# 模型缓存功能

## 功能概述

为了提升用户体验和减少API调用次数，系统实现了模型列表的内存缓存机制。用户登录时不再每次都从远程API获取模型列表，而是使用缓存的数据，显著提升响应速度。

## 技术实现

### 缓存结构
```go
type ModelCache struct {
    models    []ModelInfo  // 缓存的模型列表
    cacheTime time.Time    // 缓存时间戳
    mutex     sync.RWMutex // 读写锁，保证并发安全
}
```

### 缓存策略
- **缓存时间**: 5分钟
- **存储位置**: 内存中（不持久化到文件）
- **并发安全**: 使用读写锁保护缓存数据
- **自动过期**: 超过5分钟自动失效，重新获取

## 工作流程

### 1. 首次获取模型
```
用户连接 → 检查缓存 → 缓存为空 → 从API获取 → 更新缓存 → 返回模型列表
```

### 2. 缓存命中
```
用户连接 → 检查缓存 → 缓存有效 → 直接返回缓存数据
```

### 3. 缓存过期
```
用户连接 → 检查缓存 → 缓存过期 → 从API获取 → 更新缓存 → 返回模型列表
```

## 核心函数

### GetAvailableModels()
主要的模型获取函数，自动处理缓存逻辑：
- 首先检查缓存是否有效
- 如果缓存有效，直接返回缓存数据
- 如果缓存无效，调用fetchAndCacheModels()

### fetchAndCacheModels()
从远程API获取模型并更新缓存：
- 发送HTTP请求到API端点
- 解析响应数据
- 更新全局缓存
- 返回模型列表

### ClearModelCache()
清空缓存（用于测试或强制刷新）：
```go
func ClearModelCache() {
    modelCache.mutex.Lock()
    modelCache.models = nil
    modelCache.cacheTime = time.Time{}
    modelCache.mutex.Unlock()
}
```

### GetCacheInfo()
获取缓存状态信息（用于调试）：
```go
func GetCacheInfo() (int, time.Duration, bool) {
    // 返回：模型数量、缓存年龄、是否有效
}
```

## 性能优势

### 响应时间对比
- **无缓存**: 每次连接需要200-1000ms获取模型列表
- **有缓存**: 缓存命中时响应时间 < 1ms

### API调用减少
- **无缓存**: 每个用户连接都调用API
- **有缓存**: 5分钟内多个用户共享一次API调用

### 并发处理
- 使用读写锁，支持多个用户同时读取缓存
- 只有在更新缓存时才需要写锁

## 配置参数

### 缓存有效期
```go
const cacheExpiration = 5 * time.Minute
```

可以根据需要调整缓存时间：
- 更短时间：数据更新及时，但API调用更频繁
- 更长时间：减少API调用，但数据可能不够及时

## 测试验证

### 自动化测试
运行测试脚本：
```bash
./scripts/test_model_cache.sh
```

### 手动测试步骤
1. **第一次连接**：观察模型加载时间（较慢，从API获取）
2. **立即第二次连接**：观察模型加载时间（很快，使用缓存）
3. **等待6分钟后连接**：观察模型加载时间（较慢，缓存过期重新获取）

### 验证指标
- 首次加载时间：200-1000ms
- 缓存命中时间：< 10ms
- 缓存过期后重新加载：200-1000ms

## 错误处理

### API请求失败
- 如果远程API不可用，返回错误
- 不会影响已缓存的数据
- 下次请求时会重试

### 并发安全
- 使用读写锁防止数据竞争
- 多个goroutine可以安全地同时访问缓存

### 内存管理
- 缓存数据存储在内存中
- 程序重启时缓存会清空
- 不会无限增长，只缓存最新的模型列表

## 监控和调试

### 缓存状态查询
```go
count, age, valid := GetCacheInfo()
fmt.Printf("缓存模型数量: %d, 缓存年龄: %v, 是否有效: %t\n", count, age, valid)
```

### 日志记录
系统会在以下情况记录日志：
- 首次获取模型列表
- 缓存过期重新获取
- API请求失败

## 未来优化

### 可能的改进方向
1. **智能缓存更新**：检测到新模型时主动更新缓存
2. **分级缓存**：不同用户群体使用不同的缓存策略
3. **持久化缓存**：将缓存保存到文件，程序重启后仍可使用
4. **缓存预热**：程序启动时预先加载模型列表

### 配置化
将缓存参数移到配置文件中：
```yaml
cache:
  model_cache_duration: 5m
  enable_model_cache: true
  max_cache_size: 1000
```

## 注意事项

1. **内存使用**：缓存会占用一定内存，但通常很小（< 1MB）
2. **数据一致性**：缓存期间新增的模型不会立即显示
3. **并发限制**：大量并发请求时，第一个请求获取数据，其他请求等待
4. **错误恢复**：API错误不会清空现有缓存，保证服务可用性